{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import torch\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../preparedata/data_train.csv')\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-FOOD\": 1,\n",
    "    \"I-FOOD\": 2,\n",
    "    \"B-COMMAND_1\": 3,\n",
    "    \"I-COMMAND_1\": 4,\n",
    "    \"B-COMMAND_2\": 5,\n",
    "    \"I-COMMAND_2\": 6,\n",
    "    \"B-TABLE\": 7,\n",
    "    \"I-TABLE\": 8,\n",
    "    \"B-QUESTION\": 9,\n",
    "    \"I-QUESTION\": 10\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "for sentence_id, group in df.groupby('SENTENCE'):\n",
    "    tokens = group['WORD_TOKENIZE'].tolist()\n",
    "    tags = group['TAG'].tolist()\n",
    "    sentences.append(tokens)\n",
    "    labels.append(tags)\n",
    "\n",
    "encoded_labels = [[label2id[label] for label in sentence] for sentence in labels]\n",
    "\n",
    "train_sentences, test_sentences, train_labels, test_label = train_test_split(sentences, encoded_labels, test_size=0.2)\n",
    "\n",
    "data_traning = {\n",
    "    \"tokens\" :train_sentences,\n",
    "    \"labels\": train_labels\n",
    "}\n",
    "\n",
    "data_testing = {\n",
    "    \"tokens\" :test_sentences,\n",
    "    \"labels\": test_label\n",
    "}\n",
    "\n",
    "print(\"ตัวอย่าง sentence:\", data_traning['labels'][0])\n",
    "print(\"ตัวอย่าง sentence:\", data_traning['tokens'][0])\n",
    "print(\"ตัวอย่าง sentence:\", data_testing['labels'][0])\n",
    "print(\"ตัวอย่าง sentence:\", data_testing['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Geotrend/bert-base-th-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    # id2label=id2label,\n",
    "    # label2id=label2id\n",
    ")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True)\n",
    "\n",
    "    label_all_tokens = True\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_encodings = tokenize_and_align_labels(data_traning)\n",
    "test_encodings = tokenize_and_align_labels(data_testing)\n",
    "\n",
    "# สร้าง Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict(train_encodings)\n",
    "test_dataset = Dataset.from_dict(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens_and_labels(tokens, labels):\n",
    "    merged_tokens = []\n",
    "    merged_labels = []\n",
    "\n",
    "    current_word = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        token = str(token)\n",
    "\n",
    "        if token in ['[CLS]', '[SEP]']:\n",
    "            continue\n",
    "\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word.append(token[2:])\n",
    "        else:\n",
    "\n",
    "            if current_word:\n",
    "                merged_tokens.append(\"\".join(current_word))\n",
    "                merged_labels.append(current_label)\n",
    "\n",
    "            current_word = [token]\n",
    "            current_label = label\n",
    "\n",
    "    if current_word:\n",
    "        merged_tokens.append(\"\".join(current_word))\n",
    "        merged_labels.append(current_label)\n",
    "    decoded_labels = [id2label[id] for id in merged_labels]\n",
    "\n",
    "    return merged_tokens, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "print(accelerate.__version__)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn_crfsuite.utils import flatten\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "y_pred = np.argmax(predictions.predictions, axis=2)\n",
    "y_true = test_dataset['labels']\n",
    "\n",
    "\n",
    "y_pred_flat = flatten(y_pred)\n",
    "y_true_flat = flatten(y_true)\n",
    "\n",
    "sorted_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(classification_report(y_true_flat, y_pred_flat, labels=sorted_labels, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=sorted_labels)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted_labels, yticklabels=sorted_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for NER (Encoded Labels as Numbers)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_filtered = [true_label for true_label in y_true_flat if true_label != -100]\n",
    "y_pred_filtered = [pred_label for true_label, pred_label in zip(y_true_flat, y_pred_flat) if true_label != -100]\n",
    "\n",
    "y_pred_encoded = y_pred_text = [id2label[label] for label in y_pred_filtered]\n",
    "y_true_encoded = y_pred_text = [id2label[label] for label in y_true_filtered]\n",
    "\n",
    "sorted_labels =  ['B-COMMAND_1', 'I-COMMAND_1', 'B-COMMAND_2','I-COMMAND_2', 'B-FOOD', 'I-FOOD', 'B-QUESTION', 'I-QUESTION', 'B-TABLE', 'I-TABLE']\n",
    "print(classification_report(y_true_encoded, y_pred_encoded, labels=sorted_labels, zero_division=0))\n",
    "cm = confusion_matrix(y_true_encoded, y_pred_encoded, labels=sorted_labels)\n",
    "\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt='.1f', xticklabels=sorted_labels, yticklabels=sorted_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix as Percentage (%)')\n",
    "plt.savefig('confusion_matrix_tranformer_model.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./model')\n",
    "tokenizer.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model'\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "text = \"ขนมปังสังขยาโต๊ะ 3 แล้วก็ของโต๊ะ 4 ชั้นได้เตรียมไว้ให้เสร็จแล้วหรือยังครับผม\"\n",
    "word_cut = word_tokenize(text, keep_whitespace=False)\n",
    "\n",
    "inputs = tokenizer(word_cut, truncation=True, is_split_into_words=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "print(len(inputs[\"input_ids\"][0].tolist()), len(predicted_labels[0].tolist()))\n",
    "\n",
    "predicted_labels_list = predicted_labels[0].tolist()\n",
    "inputs_list = inputs[\"input_ids\"][0].tolist()\n",
    "\n",
    "text, tag_ner = merge_tokens_and_labels(tokenizer.convert_ids_to_tokens(inputs_list), predicted_labels_list)\n",
    "print(text)\n",
    "print(tag_ner)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
